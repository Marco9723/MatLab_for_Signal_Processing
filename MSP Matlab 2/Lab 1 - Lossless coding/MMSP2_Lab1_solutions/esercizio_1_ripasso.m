%% MMSP2 - Lab 1
%  Exercise 1 - Audio signal encoding

clearvars
close all
clc

%% 1) Load file 'pf.wav' and plot it. 
%%    Check that its values are included between [-1,+1]
[x,Fs]=audioread('pf.wav');
figure()
plot(x)
xlabel('freq samples')
ylabel('magnitude')


%% 2) Take the first 60 s of the file and rescale its values 
%% between [0 255], rounding to the nearest integer
dur=60;
x60=x(1:dur*Fs);
min_in=-1;
max_in=1;
min_out=0;
max_out=255;

x60_255=round((x60-min_in)/(max_in-min_in)*(max_out-min_out)+min_out);


%% 3) Convert each value into its binary representation over 8 bit
%%    hint: use the function de2bi() or dec2bin() - '0' if you don't have the Communications System toolbox
x60_255_bi=dec2bin(x60_255)-'0';


%% 4) Display the entropy of the binary source that has generated the signal
%%    hint: use the function hist() to compute the probability distribution
alphabet_bi=[0:1];
d_bi=hist(x60_255_bi(:),alphabet_bi);
p_bi=d_bi/sum(d_bi);

figure
bar(alphabet_bi,p_bi)
xlabel('symbol')
ylabel('probability')

H_bi = -sum(p_bi .* log2(p_bi));


%% 5) Consider the signal as generated by a finite source with alphabet [0 255].
%%    Plot the normalized histogram and compute the entropy.
%%    hint: avoid evaluating log2(0) when computing the entropy!
alphabet_255=[0:255];
d_255=hist(x60_255,alphabet_255);
p_255=d_255/sum(d_255);

figure
bar(alphabet_255,p_255);
xlabel('symbol')
ylabel('probability')

H_255 = -sum(p_255(d_255>0) .* log2(p_255(d_255>0)));


%% 6) Consider the signal as generated by a source with memory=1.
%%    Compute the conditional entropy
j=x60_255;
k=[0;x60_255(1:end-1)];
d_joint=hist3([j,k], {alphabet_255,alphabet_255});
p_joint=d_joint/sum(d_joint(:));

figure()
imagesc(db(p_joint)),axis xy
title('Joint PMF')
ylabel('jsymbols')
xlabel('ksymbols')

H_joint = -sum(sum(p_joint(d_joint>0) .* log2(p_joint(d_joint>0))));








